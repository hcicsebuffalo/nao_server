{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, abort, request, jsonify , Response\n",
    "from tempfile import NamedTemporaryFile\n",
    "#from helper_chatgpt import gptResponse\n",
    "import time\n",
    "\n",
    "AUDIO_RECOG = True\n",
    "FACE_RECOG = True\n",
    "TRANSCRIBE = True\n",
    "EMOTION = True\n",
    "WAKE_WORD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/sunilruf/miniconda3/envs/bio2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the model: No module named 'timm'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2906642/1258817342.py\", line 3, in <module>\n",
      "    model = torch.load(PATH,map_location=torch.device('cpu'))\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/csgrad/sunilruf/miniconda3/envs/bio2/lib/python3.11/site-packages/torch/serialization.py\", line 1026, in load\n",
      "    return _load(opened_zipfile,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/csgrad/sunilruf/miniconda3/envs/bio2/lib/python3.11/site-packages/torch/serialization.py\", line 1438, in _load\n",
      "    result = unpickler.load()\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/csgrad/sunilruf/miniconda3/envs/bio2/lib/python3.11/site-packages/torch/serialization.py\", line 1431, in find_class\n",
      "    return super().find_class(mod_name, name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'timm'\n"
     ]
    }
   ],
   "source": [
    "PATH='/home/csgrad/sunilruf/nao_server/models/emotions.pt'\n",
    "try:\n",
    "    model = torch.load(PATH,map_location=torch.device('cpu'))\n",
    "    model=model.to(device)\n",
    "    model.eval()\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error loading the model:\", e)\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch    \n",
    "whispher_model = whisper.load_model(\"medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/sunilruf/miniconda3/envs/bio2/lib/python3.11/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Model\n",
    "from pyannote.audio import Inference\n",
    "from scipy.spatial.distance import cdist\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/c6335d8f1cd77b30084387468a6cf26fea90009b/pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/c6335d8f1cd77b30084387468a6cf26fea90009b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.2.0. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.2.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "audio_recog_model = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=\"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\")\n",
    "inference = Inference(audio_recog_model, window=\"whole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_Audio_embeddings = {}\n",
    "\n",
    "    folder_path= \"persons\"\n",
    "    for person_folder in os.listdir(folder_path):\n",
    "        person_folder_path = os.path.join(folder_path, person_folder)\n",
    "        if not os.path.isdir(person_folder_path):\n",
    "            continue  # Skip if it's not a folder\n",
    "\n",
    "        # Path to the face image file\n",
    "        audio_folder_path = os.path.join(person_folder_path, \"audio\")\n",
    "        sample_audio_path = os.path.join(audio_folder_path, \"sample.wav\")\n",
    "\n",
    "        if os.path.isfile(sample_audio_path):\n",
    "            cal_audio_embedding = inference(sample_audio_path)\n",
    "            cal_audio_embedding = cal_audio_embedding.reshape(1,512)\n",
    "            stored_Audio_embeddings[person_folder.lower()] = cal_audio_embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
